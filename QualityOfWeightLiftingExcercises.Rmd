---
title: 'Qualitative Activity of Weight lifting excercises'
author: "M. Jung"
date: "February, 2016"
output:
  html_document: default
  pdf_document:
    fig_caption: yes
---

## Executive Summary
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.    
On the following Internet Site you can find data from accelerometers to quantify the quality of an exercise:     
http://groupware.les.inf.puc-rio.br/har Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.    
Based on these given data the goal of this project is to predict the manner in which Weight lifting exercises are done.
So I built different prediction models, used cross validation, calculated the expected out of sample erro and predicted how well the exercise is done on a small given data set.    
As result I used the model calculated with Random Forest for the prediction. The reason was the best expected accuracy.
The prediction on the small given data set was calculated with this model and meets the expected outcome on the Coursera Course Project Prediction Quiz.

## Loading the data   
First I load the collected data ("pml-training.csv"") and the small data set for the prediction ("pml-testing.csv").

```{r getData, cache=TRUE}
urltrain <-  
  "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
pmltrain <- read.csv(url(urltrain))
urltest <-  
  "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
pmltest <- read.csv(url(urltest))
```

"pml-testing" only has twenty rows and do not include the classification how well the exercise is done (Attribute "classes").

So for testing the model and to estimate the expected accuracy I create a training set (75% of the data) and a test set (25% of the data) from the pml-training data.

```{r getTrainTest, cache=TRUE}
# create own training and testing data from pmltrain
library(caret)
set.seed(123)
trainingRows <- createDataPartition(pmltrain$classe,
                                    p = .75,
                                    list= FALSE)
training <- pmltrain[trainingRows,]
testing <- pmltrain[-trainingRows,]  
```

## Feature Selection
By having a short look on the data you recognize that there are a lot of attributes and not all attributes are practical to do prediction for "classes".      

So let's do some feature selection on the training data.    
 
I did the following:     
- remove the first seven columns because they are not predictors    
- remove the columns with low variance     
- remove the columns with sparse values     

For the selection of the attributes with low variance I used the caret function "nearZeroVariance".     
For the selection of sparse columns I define sparse when more than 95 % of the values are not available.     

```{r AttributeSelection, cache=TRUE}
# Attribute selection
# Remove first seven attributes
training <- training[,-(1:7)]
# Remove Attributes with low variance
nearZeroVariance <- nearZeroVar(training)
training <- training[,-nearZeroVariance]
# Remove attributs with mostly NULL
NotSparse <- sapply(training, function(x) sum(is.na(x))/length(x)) <= 0.95
training <- training[, NotSparse]
```

After Feature Selection are 52 predicors and the attribute to predict, "classe", left.     

## Multiple Models
With these predictors I build a couple of models with different methods by using the caret package.
My laptop is not to bad, but I had to select some variables to get the algorithm run.     

First I built a model with a single regression tree.    

```{r rpart, cache=TRUE}
# single regression tree (CART Tree)
# tune over complexity parameter -> rpart
ctrl <- trainControl(method="cv", verboseIter=F)
set.seed(123)
rpartFit <- train(classe ~ ., 
                data = training,
                method = "rpart",
                tuneLength = 20,                 
                #complex = 1,
                trControl=ctrl
)
```

"Rpart" uses the compexity Parameter for optimization.


```{r plot rpart,fig.width=6, fig.height=4, echo=FALSE, cache=TRUE}
plot(rpartFit)
```

As second method I use "rpart2". 

```{r rpart2, cache=TRUE}
# single regression tree (CART Tree)
# tune over maximum depth -> rpart2
ctrl <- trainControl(method="cv", verboseIter=F)
set.seed(123)
rpart2Fit <- train(classe ~ ., 
                   data = training,
                    method = "rpart2",
                    tuneLength = 20,                 
                    trControl=ctrl
)
```

Rpart2 has a different optimization algorithm. It uses "max tree depth".

```{r plot rpart2,fig.width=6, fig.height=4, echo=FALSE, cache=TRUE}
plot(rpart2Fit)
```

The Third model is "Random Forest". I have choosen 3 fold cross validation as parameter.   

```{r RandomForest, cache=TRUE}
set.seed(123)
ctrl <- trainControl(method="cv", number=3, verboseIter=F)
rfFit <- train(classe ~ ., 
                data = training,
                method = "rf",
                trControl=ctrl
)
```


The last model is "boosting regression" model (method = "gbm".)
I have chosen a couple of parameters to get it run on my laptop.

```{r gbm, cache=TRUE}
# boosting regression
gbmGrid <- expand.grid(interaction.depth = 2,
                       n.trees = 10,
                       shrinkage = 0.05,
                       n.minobsinnode = 2)
set.seed(123)
gbmFit01 <- train(classe ~ ., 
                data = training,
                method = "gbm",
                tuneGrid = gbmGrid,
                verbose = F
)
```


As Summary for the different models I got the following results:    

| Model   | Parameters                | Accuracy Training | Accuracy Test| Match 20 Samples |
| --------|---------------------------|-------------------|--------------|------------------|
| rpart   | tuneLenght=20,method="cv" | 0.791             | 0.793        | 13               | 
| rpart2  | tuneLenght=20,method="cv" | 0.746             | 0.749        | 14               |
| rf      | method="cv",number=3      | 0.989             | 0.993        | 20               | 
| gbm     | interaction.depth=2,      | 0.652             | 0.652        | 9                |
|         | n.trees=10,shrinkage=0.05 |                   |              |                  |
|         | n.minobsinnode = 2        |                   |              |                  |  


So I got the best results by using a Random Forest model. 
The column "Match 20 samples" compares the prediction with the result of the project quiz. Just to show how different the results are by using a model with low accuracy.

Maybe with further optimization of the other models (different parameters) I could get better results than with the choosen parameters. Because of the very good result of the random forest model it is not worth to spend the time in trying to optimize the other models.

In the next steps I show exemplary with the Random Forest model how to get the accuracy and how to predict with the model.

First I print the results of the random forest model.

```{r print rf results, cache=TRUE}
print(rfFit$results)
```


As in the other models cross validation is used (3 repeats of 10-fold cross validation).
The best accuracy the algorithm got is "0.9888572" by mtry2 (No. of variables tried at each split). That's the accuracy I put in the table above.

Next I look at the error rate of the final model.

```{r plot rf$finalModel,fig.width=6, fig.height=4, echo=FALSE, cache=TRUE}
plot(rfFit$finalModel)
```

The black curve is the Out-of-Bag error rate. 

Here are the error rates as table. I printed the error rates for the first (1-5) and last (495-500) trees.

```{r rf error rate, cache=TRUE}
head((rfFit$finalModel$err.rate))
tail((rfFit$finalModel$err.rate))
```

The most imortant predictor in the final model is "roll_belt".

```{r plot rf importance,fig.width=6, fig.height=4, echo=FALSE, cache=TRUE}
plot(varImp(rfFit), top = 10)
```

After set I do a prediction on the testing data and built the confusion matrix.

```{r prediction and confusion matrix testing, cache=TRUE}
# use model for the testing data 
predicttest <- predict(rfFit, newdata = testing)
# Confusion Matrix on testing data
confusionMatrix(predicttest, testing$classe)
```

On the testing set I get a high accuracy of 99.27%.    

I calculate the prediction on the small given data set "pml-test" with this model and the result meets the expected outcome on the Coursera Course Project Prediction Quiz.

```{r prediction pml-test, cache=TRUE}
# use model for the "pml-test" data
predictpmltestrf <- predict(rfFit, newdata = pmltest)
```

To get the hits of the other methods I simply compare the result with the result of the random forest model (which I knew from the project quiz that these results meets the expectation).

As example with the rpart model:

```{r comparison pml-test, cache=TRUE}
# use models for the "pml-test" data
# compare the results and summarize equal predictions 
sum (predict(rfFit, newdata = pmltest) == predict(rpartFit, newdata = pmltest))
```





